{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"â€œNetwork Analysis_Negative Semantic","provenance":[{"file_id":"1Mup62UklsulKdKA_9JAorrSfM8OX3JhX","timestamp":1583862939846},{"file_id":"10fVF-b-s5Vc-VaCD9zXmaBDFecBO_C1e","timestamp":1582757381970}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mC6opQwRThJO","colab_type":"text"},"source":["# Semantic Network Graph"]},{"cell_type":"code","metadata":{"id":"ejADAkPzhM7w","colab_type":"code","outputId":"66bc6dbb-9706-404f-fbcc-bf221d6746f5","executionInfo":{"status":"ok","timestamp":1583868713420,"user_tz":360,"elapsed":307,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R50E_n5qrVZd","colab_type":"code","colab":{}},"source":["from textblob import TextBlob\n","from time import sleep"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H58USD6ih8DB","colab_type":"code","outputId":"b7c60f3d-fba1-4e8e-fc4e-78ebf1f00360","executionInfo":{"status":"ok","timestamp":1583868717934,"user_tz":360,"elapsed":445,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["import nltk\n","wn = nltk.WordNetLemmatizer()\n","ps = nltk.PorterStemmer()\n","import glob\n","import os\n","import re\n","import shutil\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","import csv\n","from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","import string\n","import itertools\n","import zipfile\n","import json\n","punctuation = string.punctuation\n","stopwordsset = set(stopwords.words(\"english\"))\n","stopwordsset.add('rt')\n","stopwordsset.add(\"'s\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"veJH6jwJiCjd","colab_type":"code","colab":{}},"source":["# removing urls\n","def removeURL(text):\n","  result = re.sub(r\"http\\S+\",\"\",text)\n","  return result\n","\n","# extracting contextual words from sentences\n","# tokenizing is taking out all the words in a sentence and turning it into tokens/words\n","def tokenize(text):\n","  #lower case\n","  text = text.lower()\n","  #split into individual words\n","  words = word_tokenize(text)\n","  return words\n","#stem - peaches : peach : reduce the number of repeated words\n","def stem(tokenizedtext):\n","  rootwords = []\n","  for aword in tokenizedtext:\n","    aword = ps.stem(aword)\n","    rootwords.append(aword)\n","  return rootwords\n","#removes useless words such as a, an, the\n","def stopWords(tokenizedtext):\n","  goodwords = []\n","  for aword in tokenizedtext:\n","    if aword not in stopwordsset:\n","      goodwords.append(aword)\n","  return goodwords\n","# feature reduction. taking words and getting their roots and graphing only the root words\n","def lemmatizer(tokenizedtext):\n","  lemmawords = []\n","  for aword in tokenizedtext:\n","    aword = wn.lemmatize(aword)\n","    lemmawords.append(aword)\n","  return lemmawords\n","#inputs a list of tokens and returns a list of unpunctuated tokens/words\n","def removePunctuation(tokenizedtext):\n","  nopunctwords = []\n","  for aword in tokenizedtext:\n","    if aword not in punctuation:\n","      nopunctwords.append(aword)\n","  cleanedwords = []\n","  for aword in nopunctwords:\n","    aword = aword.translate(str.maketrans('', '', string.punctuation))\n","    cleanedwords.append(aword)\n","  return cleanedwords"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U5Je7_vflC8A","colab_type":"code","outputId":"5eeb9927-d3e7-4e08-9f42-bccd40fba13e","executionInfo":{"status":"ok","timestamp":1583868722881,"user_tz":360,"elapsed":302,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["tweetzipfiles=glob.glob('drive/My Drive/Colab Notebooks/Network Analysis/*.zip')\n","tweetzipfiles"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['drive/My Drive/Colab Notebooks/Network Analysis/@Dior.zip',\n"," 'drive/My Drive/Colab Notebooks/Network Analysis/@LouisVuitton.zip',\n"," 'drive/My Drive/Colab Notebooks/Network Analysis/@CHANEL.zip']"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"MfOwZdo5paK5","colab_type":"text"},"source":["# Negative Semantic"]},{"cell_type":"code","metadata":{"id":"iMWhMhBas-aJ","colab_type":"code","colab":{}},"source":["from textblob import TextBlob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGDEihHApHzR","colab_type":"code","outputId":"a54af3c2-e84f-45d6-ddd2-acc1ef4ccfc2","executionInfo":{"status":"ok","timestamp":1583868750813,"user_tz":360,"elapsed":24429,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":539}},"source":["from time import sleep\n","uniquewords = {}\n","count = 0\n","\n","for tweetzipfile in tweetzipfiles:\n","  zf = zipfile.ZipFile(tweetzipfile)\n","  for i, obj in enumerate(zf.infolist()):\n","    tweetjson = json.load(zf.open(obj))\n","    count += 1\n","    if count % 1000 ==0:\n","      print(count)\n","\n","    text=tweetjson['text']\n","    textannotation = TextBlob(text)\n","    sentimentscore = textannotation.sentiment.polarity\n","    if sentimentscore < 0:\n","    # language pre processing: clean the tweet\n","      nourlstext= removeURL(text)\n","      tokenizedtext = tokenize(nourlstext)\n","      nostopwordstext = stopWords(tokenizedtext)\n","      lemmatizedtext = lemmatizer(nostopwordstext)\n","      nopunctext = removePunctuation(lemmatizedtext)\n","\n","    for aword in nopunctext:\n","        if aword in uniquewords:\n","            uniquewords[aword] +=1\n","        if aword not in uniquewords:\n","            uniquewords[aword] =1\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","10000\n","11000\n","12000\n","13000\n","14000\n","15000\n","16000\n","17000\n","18000\n","19000\n","20000\n","21000\n","22000\n","23000\n","24000\n","25000\n","26000\n","27000\n","28000\n","29000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E-yo9ESgpiUp","colab_type":"code","colab":{}},"source":["for aword in nopunctext:\n","        if aword in uniquewords:\n","            uniquewords[aword] +=1\n","        if aword not in uniquewords:\n","            uniquewords[aword] =1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_vUuiNwbp6Hr","colab_type":"code","colab":{}},"source":["# list of unqiue words:key-word; value=number of times the word appears\n","wordstoinclude = set()\n","wordcount=0\n","for aword in uniquewords:\n","    if uniquewords[aword]> 25:\n","      wordcount += 1\n","      wordstoinclude.add(aword)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MH4WDsM6qqIf","colab_type":"code","outputId":"d932d137-6b13-4ed0-883e-045c88207f84","executionInfo":{"status":"ok","timestamp":1583868826248,"user_tz":360,"elapsed":240,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(wordcount)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1306\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3723hZcYq989","colab_type":"code","outputId":"4ceda15b-6426-4858-df5c-723e6770cd71","executionInfo":{"status":"ok","timestamp":1583868828804,"user_tz":360,"elapsed":312,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["edgelist = open('drive/My Drive/Colab Notebooks/Network Analysis/retail.semantic.edgelist.for.gephi_segscore.csv','w')\n","csvwriter= csv.writer(edgelist)\n","\n","header=['Source','Target','Type']\n","csvwriter.writerow(header)\n","\n","print('Writing Edge List')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Writing Edge List\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_UndFsDFpqlm","colab_type":"code","outputId":"55cbf297-0375-403f-be18-346624dfb0fa","executionInfo":{"status":"ok","timestamp":1583868857941,"user_tz":360,"elapsed":26981,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":539}},"source":["uniquewords = {}\n","count = 0\n","\n","for tweetzipfile in tweetzipfiles:\n","  zf = zipfile.ZipFile(tweetzipfile)\n","  for i, obj in enumerate(zf.infolist()):\n","    tweetjson = json.load(zf.open(obj))\n","    count += 1\n","    if count % 1000 ==0:\n","      print(count)\n","\n","    text=tweetjson['text']\n","    textannotation = TextBlob(text)\n","    sentimentscore = textannotation.sentiment.polarity\n","    if sentimentscore < 0:\n","      nourlstext= removeURL(text)\n","      tokenizedtext = tokenize(nourlstext)\n","      nostopwordstext = stopWords(tokenizedtext)\n","      lemmatizedtext = lemmatizer(nostopwordstext)\n","      nopunctext = removePunctuation(lemmatizedtext)\n","\n","    goodwords = []\n","    for aword in nopunctext:\n","        if aword in wordstoinclude:\n","          goodwords.append(aword.replace(',',''))\n","\n","    allcombos =itertools.combinations(goodwords,2)\n","    for acombo in allcombos:\n","        row =[]\n","        for anode in acombo:\n","          row.append(anode)\n","        row.append('Undirected')\n","        csvwriter.writerow(row)\n","\n","edgelist.close()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","10000\n","11000\n","12000\n","13000\n","14000\n","15000\n","16000\n","17000\n","18000\n","19000\n","20000\n","21000\n","22000\n","23000\n","24000\n","25000\n","26000\n","27000\n","28000\n","29000\n"],"name":"stdout"}]}]}