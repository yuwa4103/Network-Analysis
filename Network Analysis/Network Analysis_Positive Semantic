{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of â€œNetwork Analysis_Positive Semantic","provenance":[{"file_id":"1Mup62UklsulKdKA_9JAorrSfM8OX3JhX","timestamp":1583862939846},{"file_id":"10fVF-b-s5Vc-VaCD9zXmaBDFecBO_C1e","timestamp":1582757381970}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mC6opQwRThJO","colab_type":"text"},"source":["# Semantic Network Graph"]},{"cell_type":"code","metadata":{"id":"ejADAkPzhM7w","colab_type":"code","outputId":"0b9b3755-b2e9-418f-b48f-cc540546ba9b","executionInfo":{"status":"ok","timestamp":1583953630625,"user_tz":360,"elapsed":590,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R50E_n5qrVZd","colab_type":"code","colab":{}},"source":["from textblob import TextBlob\n","from time import sleep"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H58USD6ih8DB","colab_type":"code","outputId":"073ca1bb-a6ce-47c6-90a7-2547a8fc37f3","executionInfo":{"status":"ok","timestamp":1583953635514,"user_tz":360,"elapsed":528,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["import nltk\n","wn = nltk.WordNetLemmatizer()\n","ps = nltk.PorterStemmer()\n","import glob\n","import os\n","import re\n","import shutil\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","import csv\n","from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","import string\n","import itertools\n","import zipfile\n","import json\n","punctuation = string.punctuation\n","stopwordsset = set(stopwords.words(\"english\"))\n","stopwordsset.add('rt')\n","stopwordsset.add(\"'s\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"veJH6jwJiCjd","colab_type":"code","colab":{}},"source":["# removing urls\n","def removeURL(text):\n","  result = re.sub(r\"http\\S+\",\"\",text)\n","  return result\n","\n","# extracting contextual words from sentences\n","# tokenizing is taking out all the words in a sentence and turning it into tokens/words\n","def tokenize(text):\n","  #lower case\n","  text = text.lower()\n","  #split into individual words\n","  words = word_tokenize(text)\n","  return words\n","#stem - peaches : peach : reduce the number of repeated words\n","def stem(tokenizedtext):\n","  rootwords = []\n","  for aword in tokenizedtext:\n","    aword = ps.stem(aword)\n","    rootwords.append(aword)\n","  return rootwords\n","#removes useless words such as a, an, the\n","def stopWords(tokenizedtext):\n","  goodwords = []\n","  for aword in tokenizedtext:\n","    if aword not in stopwordsset:\n","      goodwords.append(aword)\n","  return goodwords\n","# feature reduction. taking words and getting their roots and graphing only the root words\n","def lemmatizer(tokenizedtext):\n","  lemmawords = []\n","  for aword in tokenizedtext:\n","    aword = wn.lemmatize(aword)\n","    lemmawords.append(aword)\n","  return lemmawords\n","#inputs a list of tokens and returns a list of unpunctuated tokens/words\n","def removePunctuation(tokenizedtext):\n","  nopunctwords = []\n","  for aword in tokenizedtext:\n","    if aword not in punctuation:\n","      nopunctwords.append(aword)\n","  cleanedwords = []\n","  for aword in nopunctwords:\n","    aword = aword.translate(str.maketrans('', '', string.punctuation))\n","    cleanedwords.append(aword)\n","  return cleanedwords"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U5Je7_vflC8A","colab_type":"code","outputId":"2b60c669-be3d-4bb9-e26d-09a6b2a52a8e","executionInfo":{"status":"ok","timestamp":1583953640518,"user_tz":360,"elapsed":820,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["tweetzipfiles=glob.glob('drive/My Drive/Colab Notebooks/Network Analysis/*.zip')\n","tweetzipfiles"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['drive/My Drive/Colab Notebooks/Network Analysis/@Dior.zip',\n"," 'drive/My Drive/Colab Notebooks/Network Analysis/@LouisVuitton.zip',\n"," 'drive/My Drive/Colab Notebooks/Network Analysis/@CHANEL.zip']"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"MfOwZdo5paK5","colab_type":"text"},"source":["# Positive Semantic"]},{"cell_type":"code","metadata":{"id":"iMWhMhBas-aJ","colab_type":"code","colab":{}},"source":["from textblob import TextBlob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGDEihHApHzR","colab_type":"code","outputId":"4de59093-3128-4a06-ec1b-3352977372b6","executionInfo":{"status":"ok","timestamp":1583953672534,"user_tz":360,"elapsed":30360,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":510}},"source":["from time import sleep\n","uniquewords = {}\n","count = 0\n","\n","for tweetzipfile in tweetzipfiles:\n","  zf = zipfile.ZipFile(tweetzipfile)\n","  for i, obj in enumerate(zf.infolist()):\n","    tweetjson = json.load(zf.open(obj))\n","    count += 1\n","    if count % 1000 ==0:\n","      print(count)\n","\n","    text=tweetjson['text']\n","    textannotation = TextBlob(text)\n","    sentimentscore = textannotation.sentiment.polarity\n","    if sentimentscore > 0:\n","    # language pre processing: clean the tweet\n","      nourlstext= removeURL(text)\n","      tokenizedtext = tokenize(nourlstext)\n","      nostopwordstext = stopWords(tokenizedtext)\n","      lemmatizedtext = lemmatizer(nostopwordstext)\n","      nopunctext = removePunctuation(lemmatizedtext)\n","\n","    \n","    for aword in nopunctext:\n","        if aword in uniquewords:\n","            uniquewords[aword] +=1\n","        if aword not in uniquewords:\n","            uniquewords[aword] =1\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","10000\n","11000\n","12000\n","13000\n","14000\n","15000\n","16000\n","17000\n","18000\n","19000\n","20000\n","21000\n","22000\n","23000\n","24000\n","25000\n","26000\n","27000\n","28000\n","29000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E-yo9ESgpiUp","colab_type":"code","colab":{}},"source":["for aword in nopunctext:\n","        if aword in uniquewords:\n","            uniquewords[aword] +=1\n","        if aword not in uniquewords:\n","            uniquewords[aword] =1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_vUuiNwbp6Hr","colab_type":"code","colab":{}},"source":["# list of unqiue words:key-word; value=number of times the word appears\n","wordstoinclude = set()\n","wordcount=0\n","for aword in uniquewords:\n","    if uniquewords[aword]> 25:\n","      wordcount += 1\n","      wordstoinclude.add(aword)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MH4WDsM6qqIf","colab_type":"code","outputId":"a1753d58-ea6a-43f7-e06b-f637d9e6ea66","executionInfo":{"status":"ok","timestamp":1583954174377,"user_tz":360,"elapsed":592,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(wordcount)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["1010\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3723hZcYq989","colab_type":"code","outputId":"1d97bcbe-42dc-4e51-ad61-aa2d55ea2f1a","executionInfo":{"status":"ok","timestamp":1583954177862,"user_tz":360,"elapsed":609,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["edgelist = open('drive/My Drive/Colab Notebooks/Network Analysis/retail.semantic.edgelist.for.gephi_segscore_positive.csv','w')\n","csvwriter= csv.writer(edgelist)\n","\n","header=['Source','Target','Type']\n","csvwriter.writerow(header)\n","\n","print('Writing Edge List')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Writing Edge List\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_UndFsDFpqlm","colab_type":"code","outputId":"b2395f27-c5b3-4dce-bdda-69d7364016e2","executionInfo":{"status":"ok","timestamp":1583954214188,"user_tz":360,"elapsed":34129,"user":{"displayName":"Yuxiang Wang","photoUrl":"","userId":"00646650536874104319"}},"colab":{"base_uri":"https://localhost:8080/","height":510}},"source":["uniquewords = {}\n","count = 0\n","\n","for tweetzipfile in tweetzipfiles:\n","  zf = zipfile.ZipFile(tweetzipfile)\n","  for i, obj in enumerate(zf.infolist()):\n","    tweetjson = json.load(zf.open(obj))\n","    count += 1\n","    if count % 1000 ==0:\n","      print(count)\n","\n","    text=tweetjson['text']\n","    textannotation = TextBlob(text)\n","    sentimentscore = textannotation.sentiment.polarity\n","    if sentimentscore > 0:\n","      nourlstext= removeURL(text)\n","      tokenizedtext = tokenize(nourlstext)\n","      nostopwordstext = stopWords(tokenizedtext)\n","      lemmatizedtext = lemmatizer(nostopwordstext)\n","      nopunctext = removePunctuation(lemmatizedtext)\n","\n","    goodwords = []\n","    for aword in nopunctext:\n","        if aword in wordstoinclude:\n","          goodwords.append(aword.replace(',',''))\n","\n","    allcombos =itertools.combinations(goodwords,2)\n","    for acombo in allcombos:\n","        row =[]\n","        for anode in acombo:\n","          row.append(anode)\n","        row.append('Undirected')\n","        csvwriter.writerow(row)\n","\n","edgelist.close()"],"execution_count":26,"outputs":[{"output_type":"stream","text":["1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","10000\n","11000\n","12000\n","13000\n","14000\n","15000\n","16000\n","17000\n","18000\n","19000\n","20000\n","21000\n","22000\n","23000\n","24000\n","25000\n","26000\n","27000\n","28000\n","29000\n"],"name":"stdout"}]}]}